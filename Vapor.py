# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L3ShqVinlLBp5phRR72vXkXwKiWpGGiW
"""

!pip install PyPDF2 nltk spacy transformers streamlit

import PyPDF2

with open('/content/drive/MyDrive/Spirax Sarco/Livro do Vapor em Português_Rev01.pdf', 'rb') as pdf_file:
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        text += page.extract_text()

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')


words = word_tokenize(text)
stop_words = set(stopwords.words('portuguese'))
words = [word for word in words if word not in stop_words]

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = 't5-base'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Criar um dataset de perguntas e respostas com base no texto extraído
# Ajustar o modelo usando este dataset

import streamlit as st

def generate_answer(question):
    input_ids = tokenizer.encode(question, return_tensors='pt')
    output = model.generate(input_ids)
    return tokenizer.decode(output[0], skip_special_tokens=True)

st.title('Pergunte ao Livro do Vapor')
user_input = st.text_input("Digite sua pergunta:")
if user_input:
    answer = generate_answer(user_input)
    st.text(answer)

import streamlit as st

st.title('Chat Livro do Vapor')

st.write('Olá, mundo!')

# Adicione mais elementos da interface aqui, como botões, gráficos, etc.

!pip install gradio

def chat_response(input_text):
    # Aqui pode incluir o processamento do input, como integração com um modelo de linguagem
    return f"Você disse: {input_text}"

import gradio as gr

# Função que será chamada ao enviar uma mensagem
def chat_response(input_text):
    return f"Você disse: {input_text}"

# Interface de chat com Gradio
interface = gr.Interface(fn=chat_response, inputs="text", outputs="text", title="Chat Interativo")

# Executar a interface
interface.launch()

import streamlit as st

def chat_response(input_text):
    return f"Você disse: {input_text}"

st.title("Chat Interativo")
user_input = st.text_input("Digite sua mensagem")
if user_input:
    st.write(chat_response(user_input))

!pip install pyngrok
!ngrok authtoken SEU_AUTHTOKEN_AQUI
!streamlit run app.py

!pip install streamlit

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# 
# def chat_response(user_input):
#     return f"Você disse: {user_input}"
# 
# st.title("Chat Interativo")
# user_input = st.text_input("Digite sua mensagem")
# 
# if user_input:
#     st.write(chat_response(user_input))

!pip install pyngrok

!ngrok authtoken <YOUR_AUTHTOKEN>

!git lfs install
!git clone https://huggingface.co/spaces/gnfilho/Livro_do_Vapor